{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "This code reads in a csv file containing data on the utilization of buses in various municipalities, preprocesses the data, and performs time series forecasting using the Simple Exponential Smoothing method, SARIMA and Exponential Smoothing for each municipality. Statsmodels and Scikit-learn libraries are used for time series analysis and evaluation.\n\nThe first few lines of code import the necessary libraries and suppress warning messages.\n\nThe next section reads in the csv file containing the data and prints the first 30 rows to check the data. It also visualizes the data using a barplot to show the usage per municipality.\n\nThe \"Data preprocessing and feature extraction\" section contains functions that extract date features from the timestamp data, resample the data in hourly basis, divide the data into train and test splits for each municipality, and find the best alpha value for each municipality. Alpha is a parameter in the Simple Exponential Smoothing method that controls the level of smoothing applied to the time series. The section also contains a function to plot the forecasted time series data.\n\nFinally, the code loops through each municipality and finds the best alpha value using the alpha_validation function, trains a Simple Exponential Smoothing model on the training data with the best alpha value, generates forecasts for the test data, and plots the forecasted time series data using the plot_prediction function.\n\nThen code repeats loops through SARIMA and Exponential Smoothing as well\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport datetime\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport itertools\n\nfrom statsmodels.tsa.holtwinters  import SimpleExpSmoothing\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.holtwinters  import ExponentialSmoothing\n\nfrom sklearn.metrics import mean_absolute_error",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "\"\"\"Read data\"\"\"\ndf = pd.read_csv(\"C:\\\\Users\\\\Serkan\\\\Downloads\\\\municipality_bus_utilization.csv\", parse_dates=['timestamp'])\n\n#Print first 30 elements to check the data\nprint(df.head(30))",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "\"\"\"Visualize data to have a better understanding\"\"\"\nsns.set_style(\"whitegrid\")\nplt.figure(figsize = (8, 4))\nsns.barplot(x = df[\"municipality_id\"], y = df[\"usage\"])\nplt.xlabel('Municipality')\nplt.ylabel('Usage')\nplt.title('Usage per municipalities')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "\"\"\"Data preprocessing and featrue extraction\"\"\"\n\n#First get every time information from data such as hours and days and check for missing values\ndef create_date_features(df):\n    df['hour'] = df.timestamp.dt.hour\n    df['month'] = df.timestamp.dt.month\n    df['day_of_month'] = df.timestamp.dt.day\n    df['day_of_year'] = df.timestamp.dt.dayofyear\n    df['week_of_year'] = df.timestamp.dt.weekofyear\n    df['day_of_week'] = df.timestamp.dt.dayofweek\n    df['year'] = df.timestamp.dt.year\n    df[\"is_wknd\"] = df.timestamp.dt.weekday // 4\n    df['is_month_start'] = df.timestamp.dt.is_month_start.astype(int)\n    df['is_month_end'] = df.timestamp.dt.is_month_end.astype(int)\n    return df\ndf = create_date_features(df)\ndf.isnull().sum() # show missing values\n\ndf.groupby([\"municipality_id\",\"hour\"]).agg({\"usage\": [\"count\", \"max\"]})\n\n# All the municipalities have the same pattern. We don't need to deal with the missing hours.",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "\"\"\"Resample data in hourly basis\"\"\"\n\ndf_resampled = pd.DataFrame()\n\ndf[\"timestamp\"] = df[\"timestamp\"].astype(str).apply(lambda x: x[:-6]).astype(\"datetime64\")\n# Get the max value for the duplicates as recommended in the task\ndf_resampled = df.groupby([\"timestamp\",\"municipality_id\"]).agg({\"usage\": \"max\"}).reset_index()\ndf_resampled.drop_duplicates([\"timestamp\",\"municipality_id\"],inplace=True)\n\ndf_resampled.head()\n\n#Store the resampled version for each municipality in order separately\ndf_ordered={}\nfor i in range(10):\n    df_ordered[i]= pd.DataFrame(data=df_resampled[df_resampled.municipality_id==i], columns=[\"timestamp\",\"usage\"]).set_index(\"timestamp\")\n    ",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#Divide the data into train and test split for each municipality\ntrain_data = {}\ntest_data = {}\ntrain_end_date = \"2017-08-04 16:00:00\"\ntest_start_date = \"2017-08-05 07:00:00\"\nfor i in range(10):\n    train_data[i] = df_ordered[i][:train_end_date]\n    test_data[i] = df_ordered[i][test_start_date:]\n    ",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "SIMPLE EXPONENTIAL SMOOTHING",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "\"\"\"SIMPLE EXPONENTIAL SMOOTHING\"\"\"\n\"\"\"Create hyperparameter finder function and plot function\"\"\"\n\n#We need to pick the best alpha for each municipality before training with a fixed alpha\n#For this reason create a function that finds the best alpha value in a range\ndef alpha_validation(train,test, alphas, step=142):\n    best_alpha, lowest_mae = None, float(\"inf\")\n    for alpha in alphas:\n        valid_model = SimpleExpSmoothing(train).fit(smoothing_level=alpha)\n        y_pred = valid_model.forecast(step)\n        mae = mean_absolute_error(test, y_pred)\n        if mae < lowest_mae: # choose the alpha with lowest mae value\n            best_alpha, lowest_mae = alpha, mae\n    print(\"Best alpha is:\", round(best_alpha, 2), \"and lowest MAE is:\", round(lowest_mae, 4))\n    return best_alpha, lowest_mae\n\ndef plot_prediction(i,y_pred):\n    plt.figure(figsize=(16, 4))\n    train_data[i][\"usage\"].plot(legend=True, label=f\"TRAIN {i}\")\n    test_data[i][\"usage\"].plot(legend=True, label=f\"TEST {i}\")\n    if \"usage\" in y_pred:\n        y_pred[\"usage\"].plot(legend=True, label=f\"PREDICTION {i}\")\n    else:\n        y_pred[\"predicted_mean\"].plot(legend=True, label=f\"PREDICTION {i}\") # This line is for the values coming from SARIMA\n    plt.xlim([datetime.date(2017,6,4), datetime.date(2017,8,20)])\n    plt.title(\"Time Series Forecasting for Bus Demands in Municipality \" +str(i))\n    plt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "alphas = np.arange(0.01, 1, 0.10)\nfor i in range(10):   \n    best_alpha, _ = alpha_validation(train_data[i],test_data[i], alphas, step=142)\n    test_model = SimpleExpSmoothing(train_data[i]).fit(smoothing_level=best_alpha)\n    y_pred = test_model.forecast(142)\n\n    y_pred.reset_index(drop=True,inplace=True)\n    y_pred=pd.DataFrame(y_pred, columns=[\"usage\"])\n    y_pred = y_pred.merge(test_data[i].reset_index()[\"timestamp\"], left_index=True, right_index=True).set_index(\"timestamp\")\n    plot_prediction(i, y_pred)\n    ",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "SARIMA",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# #For a more complex version we will try SARIMA\np = d = q = range(0, 2)\npdq = list(itertools.product(p, d, q))\nseasonal_pdq = [(x[0], x[1], x[2], 4) for x in list(itertools.product(p, d, q))]\n\ndef sarima_validation(train, pdq, seasonal_pdq):\n    best_aic, best_order, best_seasonal_order = float(\"inf\"), float(\"inf\"), None\n    for param in pdq:\n        for param_seasonal in seasonal_pdq:\n            try:\n                sarimax_model = SARIMAX(train, order=param, seasonal_order=param_seasonal)\n                results = sarimax_model.fit(disp=0)\n                aic = results.aic\n                if aic < best_aic:\n                    best_aic, best_order, best_seasonal_order = aic, param, param_seasonal\n                print('SARIMA{}x{}4 - AIC:{}'.format(param, param_seasonal, aic))\n            except:\n                continue\n    print('SARIMA{}x{}4 - AIC:{}'.format(best_order, best_seasonal_order, best_aic))\n    return best_order, best_seasonal_order  \n\nfor i in range(10):   \n    best_order, best_seasonal_order = sarima_validation(train_data[i], pdq, seasonal_pdq)\n    model = SARIMAX(train_data[i], order=best_order, seasonal_order=best_seasonal_order)\n    sarima_final_model = model.fit(disp=0)\n    y_pred_test = sarima_final_model.get_forecast(steps=142)\n\n    y_pred = y_pred_test.predicted_mean\n    mean_absolute_error(test_data[i], y_pred)\n    y_pred.reset_index(drop=True,inplace=True)\n    y_pred=pd.DataFrame(y_pred)\n    y_pred = y_pred.merge(test_data[i].reset_index()[\"timestamp\"], left_index=True, right_index=True).set_index(\"timestamp\")\n    plot_prediction(i, y_pred)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "EXPONENTIAL SMOOTHING",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#First define the function that finds hyperparameters\ndef triple_validation(train,test, abg, step=142):\n    best_alpha, best_beta, best_gamma, lowest_mae = None, None, None, float(\"inf\")\n    for params in abg:\n        valid_model = ExponentialSmoothing(train, trend=\"add\", seasonal=\"add\", seasonal_periods=10).\\\n            fit(smoothing_level=params[0], smoothing_slope=params[1], smoothing_seasonal=params[2])\n        y_pred = valid_model.forecast(step)\n        mae = mean_absolute_error(test, y_pred)\n        if mae < lowest_mae:\n            best_alpha, best_beta, best_gamma, lowest_mae = params[0], params[1], params[2], mae\n        print([round(params[0], 2), round(params[1], 2), round(params[2], 2), round(mae, 2)])\n\n    print(\"Best_alpha is:\", round(best_alpha, 2), \" Best_beta is:\", round(best_beta, 2), \" Best_gamma is:\", round(best_gamma, 2),\n          \"and Lowest_mae is:\", round(lowest_mae, 4))\n\n    return best_alpha, best_beta, best_gamma, lowest_mae   \n    \nalphas = betas = gammas = np.arange(0.01, 1, 0.20)\nabg = list(itertools.product(alphas, betas, gammas))\nfor i in range(10):\n    best_alpha, best_beta, best_gamma, _ = triple_validation(train_data[i], test_data[i], abg, step=142)    \n    test_model = ExponentialSmoothing(train_data[i], trend=\"add\", seasonal=\"add\", seasonal_periods=90).fit(smoothing_level=best_alpha, smoothing_slope=best_beta, smoothing_seasonal=best_gamma)\n\n    y_pred = test_model.forecast(142)    \n\n    y_pred.reset_index(drop=True,inplace=True)\n    y_pred=pd.DataFrame(y_pred, columns=[\"usage\"])\n    y_pred = y_pred.merge(test_data[i].reset_index()[\"timestamp\"], left_index=True, right_index=True).set_index(\"timestamp\")\n    plot_prediction(i, y_pred)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "In the end by looking at the results we see that none of the functions actually give the desired results. But the order from the best to the worst is Exponential Smoothing, SARIMA, Simple Exponential Smoothing.",
      "metadata": {}
    }
  ]
}